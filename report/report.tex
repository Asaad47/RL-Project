\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage[preprint]{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}        % math
\usepackage{graphicx}

% Define todo command
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\title{RL for PySuperTuxKart}

\author{%
  Asaad Mohammedsaleh \\
  Computer Science Program\\
  King Abdullah University of Science and Technology\\
  Thuwal, Saudi Arabia \\
  \texttt{asaad.mohammedsaleh@kaust.edu.sa} \\
}


\begin{document}


\maketitle


\begin{abstract}
  This is a report on the implementation of reinforcement learning approaches for PySuperTuxKart 
  as part of Spring 2025 semester course CS294X: Introduction to Reinforcement Learning.
  In this report, we will discuss the implementation of Q-learning and DQN for PySuperTuxKart.
  The code for the implementation can be found in the GitHub repository \url{https://github.com/Asaad47/RL-Project}.
\end{abstract}


\section{Introduction}

In this project, the task is to use reinforcement learning techniques to train an agent to play PySuperTuxKart. 
In the homework assignment 6, we implemented a simple controller that uses a pre-defined policy to play the game.
I used this controller as an initial point to implement a more sophisticated controller using reinforcement learning.
Throughout this report, we will discuss the implementation details of expanding on the simple manual controller using Q-learning, and 
we will also discuss the implementation details using DQN on this task, inspired by the Atari paper \citep{mnih, mnih2}.

\subsection{PySuperTuxKart Installation}

The package \texttt{pystk} (PySuperTuxKart) requires x86\_64 architecture. To run the game locally on my M2 Macbook Air, I used \texttt{Rosetta} 
to emulate x86\_64 architecture of \texttt{Ghostty} terminal app and initialized \texttt{Conda} environment to handle x86\_64 packages.
However, whenever I run python scripts with \texttt{pystk} package, I get the following warning:

\begin{verbatim}
    Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 
(Intel(R) SSE4.2) enabled only processors has been deprecated. Intel 
oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector 
Extensions (Intel(R) AVX) instructions.
\end{verbatim}

In addition, I get the following error the first time I run a \texttt{pystk} script:

\begin{verbatim}
OMP: Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib 
    already initialized.
OMP: Hint This means that multiple copies of the OpenMP runtime have been 
    linked into the program. That is dangerous, since it can degrade 
    performance or cause incorrect results. The best thing to do is to 
    ensure that only a single OpenMP runtime is linked into the process, 
    e.g. by avoiding static linking of the OpenMP runtime in any library. 
    As an unsafe, unsupported, undocumented workaround you can set the 
    environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to 
    continue to execute, but that may cause crashes or silently produce 
    incorrect results. For more information, please see 
    http://www.intel.com/software/products/support/.
[1]    49696 abort      python DQN.py --track lighthouse --mode test --verbose
\end{verbatim}

I avoid this error by running the following command:

\begin{verbatim}
    export KMP_DUPLICATE_LIB_OK=TRUE
\end{verbatim}

However, I think this is not the best solution but I did not spend more time on this issue. I think this also creates
differences in computations between different machines, and I noticed that when testing runs locally versus on Ibex. 
I will discuss the results in the experiments section.

\subsection{PySuperTuxKart}

The game is a 3D racing game where the goal is to navigate a kart through the track 
while avoiding obstacles. The game is played from a third-person perspective.

The game documentation \citep{pystk} provides multiple settings for the image quality through the \texttt{pystk.GraphicsConfig}
class. The available settings are:

\begin{itemize}
    \item \texttt{hd()}: "High-definitaiton graphics settings"
    \item \texttt{ld()}: "Low-definition graphics settings"
    \item \texttt{sd()}: "Standard-definition graphics settings"
    \item \texttt{none()}: "Disable graphics and rendering"
\end{itemize}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{resolution.png}
    \caption{Different resolutions of the game. From left to right: \texttt{hd()}, \texttt{sd()}, \texttt{ld()}}
    \label{resolution}
\end{figure}


Figure \ref{resolution} shows the different resolutions of the game. To speed up the training, I used the \texttt{ld()} setting during training, and the \texttt{hd()} setting during testing for the Q-learning agent. 
I avoided using the \texttt{none()} setting because as the game is not rendered, training shows incorrect results. However, I used the \texttt{hd()} setting
for the DQN agent because the training needs to be exposed to the high quality images reflecting the actual game.

A race track has a start line and a finish line, and the \texttt{kart} object can track the distance from the start line using the \texttt{distance\_down\_track} attribute. 
In the initial code given for the homework, \texttt{kart.overall\_distance / track.length} is used to check if the kart has finished the race. However, the \texttt{overall\_distance}
attribute gives negative values when the kart just starts the race, giving incorrect results for \texttt{kart.overall\_distance / track.length} in the beginning of the race with a value close to -1.
\texttt{distance\_down\_track / track.length} gives a correct range from 0 to 1, and is used in the code for tracking the progress of the kart. Checking for finished race is not changed and is
 still using \texttt{overall\_distance / track.length}.

\section{Q-learning}

Under this section, we will discuss the implementation of two Q-learning agents:
\begin{itemize}
    \item \texttt{simple\_RL\_controller}: A simple Q-learning agent that only decides on steering angle out of three discrete actions (forward, left, right).
    \item \texttt{discrete\_RL\_controller}: An extended version of the simple Q-learning agent that also decides on acceleration, brake, drift, and nitro.
\end{itemize}

The main differences between the agents are how the state space, action space, and reward function are defined. Once these are defined, the Q-learning algorithm is the same. Whenever a new action is taken, the Q-value is updated using the formula from Lecture 10 notes \citep{lecnotes}:

\begin{equation}
    Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
\end{equation}

where $s$ is the current state, $s'$ is the next state, $a$ is the action, $r$ is the reward, $\gamma$ is the discount factor, and $\alpha$ is the learning rate.

\subsection{Simple Q-learning agent}

The simple Q-learning agent motivation was to minimally enhance the performance of a manual controller that uses a pre-defined policy to play the game and make sure the kart can finish the race with reinforcement learning.

\paragraph{State space}

The state space is defined as the x-coordinate of the aim point as defined in the homework assignment 6. The state space then is a single value between 0 and 127.

\paragraph{Action space}

The action space is defined as the steering angle, which can take 3 discrete values: -1, 0, 1. All other action attributes are fixed as follows: (acceleration, brake, drift, nitro) = (0.8, False, False, False).

\paragraph{Reward function}

The reward function has three components: \texttt{reward\_steer}, \texttt{reward\_rescue}, and \texttt{reward\_done}. The reward function is the sum of the three components.

\begin{equation}
    \texttt{reward\_steer} = \begin{cases}
        -|1 - \frac{2x}{\texttt{IMG\_WIDTH}}| & \text{if action steer is in the opposite direction of the aim point} \\
        0.01 & \text{otherwise}
    \end{cases}
\end{equation}

The motivation for the reward steer component is to encourage the kart to steer towards the aim point. The reward is 0.01 if the steer is aligned with the aim point.

\texttt{reward\_rescue} is $-1$ if the kart is rescued, and $0$ otherwise.

\texttt{reward\_done} is $10$ if the kart has finished the race, and $0$ otherwise.


\subsection{Discrete Q-learning agent}

The discrete Q-learning agent is an extension of the simple Q-learning agent that also decides on acceleration, brake, drift, and nitro.

\paragraph{State space}

The state space is composed of the x-coordinate of the aim point as previously defined in the simple Q-learning agent, and the percentage of the distance down the track. The state space then is a pair of two values: $(x, d)$, where $x$ can take values from 0 to 127, and $d$ can take values from 0 to 99.

\paragraph{Action space}

The action space is composed of the steering angle, acceleration, brake, drift, and nitro. The action space then is a tuple of five values: $(\texttt{steer}, \texttt{accel}, \texttt{brake}, \texttt{drift}, \texttt{nitro})$.

\begin{itemize}
    \item \texttt{steer}  $\in \{-1, 0, 1\}$
    \item \texttt{accel} $\in \{0.8, 0.5, 0.05\}$
    \item \texttt{brake} $\in \{False, True\}$
    \item \texttt{drift} $\in \{False, True\}$
    \item \texttt{nitro} $\in \{False, True\}$
\end{itemize}

The total number of actions is $3 \times 3 \times 2 \times 2 \times 2 = 72$, and the size of (state, action) space is $128 \times 100 \times 72 = 92160$.

\paragraph{Reward function}
The reward function also has three components: \texttt{reward\_steer}, \texttt{reward\_rescue}, and \texttt{reward\_done}. The reward function is the sum of the three components.

\texttt{reward\_steer} is defined as previously in the simple Q-learning agent, but \texttt{reward\_rescue} and \texttt{reward\_done} are defined as follows:

\begin{equation}
    \texttt{reward\_rescue} = \begin{cases}
        -1 & \text{if the kart is rescued} \\
        \frac{d}{1000} & \text{otherwise}
    \end{cases}
\end{equation}

where $d$ is the percentage of the distance down the track. The motivation for this reward is to encourage the kart to move forward to get higher rewards.

\begin{equation}
    \texttt{reward\_done} = \max(0.1, 10 - \frac{t}{100})
\end{equation}

where $t$ is the number of steps taken to finish the race. The motivation for this reward is to encourage the kart to finish the race in less steps.

\subsection{Implementation details}

Note that in both implementations, the Q-values are initialized to 0. During training, the agent follows an $\epsilon$-greedy policy to balance exploration and exploitation, where $\epsilon$ is the probability of taking a random action and linearly decays with the number of training steps. In particular, the $\epsilon$ is initialized to 1.0 and decays linearly to 0.1 after 1000 training steps and remains constant at 0.1 after that.

It has been observed that the kart can get stuck in a position where it cannot move forward even if the agent is trying to move it. To avoid the issue of updating the Q-values of the stuck state for long periods of time, I added a maximum threshold for the kart being in the same distance down the track. If the kart is in the same distance down the track for more than 60 steps, the kart is considered stuck and the episode is terminated. The choice of 60 steps is kind of arbitrary and can be tuned. This parameter needs to be not so large to avoid the issue of the kart getting stuck for a long time, but at the same time, it should be large enough to avoid cutting the episode too early when the kart can still move.

\section{DQN}

The implementation of Deep Q-learning Network (DQN) is based on the Atari papers \citep{mnih, mnih2}. A convolutional neural network (CNN) is used to approximate the Q-value function, where the output is a vector with the same size as the action space. The Atari paper \citep{mnih} had a an action space of 4 to 18 actions, which is smaller than the 72 actions in our case.

The update is implemented similar to the Atari paper \citep{mnih}, where the target network is updated every 1000 steps.

\paragraph{State space}

A state is a stack of 4 consecutive grayscale frames of the game, which is the same as the Atari paper. A state then would be a tensor of shape $(4, 128, 96)$.

\paragraph{Action space}

The action space is similar to the discrete Q-learning agent, where the action space is a tuple of five values: $(\texttt{steer}, \texttt{accel}, \texttt{brake}, \texttt{drift}, \texttt{nitro})$, with a total of 72 actions.

\begin{itemize}
    \item \texttt{steer}  $\in \{-1, 0, 1\}$
    \item \texttt{accel} $\in \{1.0, 0.5, 0.05\}$
    \item \texttt{brake} $\in \{False, True\}$
    \item \texttt{drift} $\in \{False, True\}$
    \item \texttt{nitro} $\in \{False, True\}$
\end{itemize}

\paragraph{Reward function}

The reward function has also three components: \texttt{reward\_move}, \texttt{reward\_rescue}, and \texttt{reward\_done}. The reward function is the sum of the three components.

\texttt{reward\_move} is $-0.5$ if the kart hasn't moved forward or has a low velocity, and $0$ otherwise.
    
\texttt{reward\_rescue} and \texttt{reward\_done} are defined as previously in the discrete Q-learning agent.

\subsection{Implementation details}

Similar neural network architecture is used as the Atari paper \citep{mnih}. The network has 3 convolutional layers with 32, 64, and 64 filters respectively followed by a dense linear layer with 512 units and a final linear layer with 72 outputs. The network is trained using the RMSprop optimizer with a learning rate of 0.0025.

The hyperparameters of interest are:
\begin{itemize}
    \item \texttt{GAMMA}: The discount factor, which is 0.99.
    \item \texttt{MEMORY\_SIZE}: The size of the replay buffer, which is 1000. It lead to out of memory error when I set it to 100000.
    \item \texttt{BATCH\_SIZE}: The batch size, which is 32.
    \item \texttt{TARGET\_UPDATE}: The number of steps between updating the target network, which is 1000.
    \item \texttt{frame\_skip = k}: The number of frames to skip between actions, which is 4.
\end{itemize}

Similar to the Q-learning agent implementation, an $\epsilon$-greedy policy is used to balance exploration and exploitation during training. Also, an episode is terminated if the kart is stuck for more than 60 steps.

\section{Experiments}

\subsection{Q-learning}

Both Q-learning agents are trained for 3000 episodes with a maximum of 2000 steps per episode. During training, an $\epsilon$-greedy policy is used for exploration and images are loaded with the \texttt{ld()} resolution. Each agent is trained on each track seperately and tested on the same track. Every 10 episodes, the Q-table is saved to the disk for later use. In the case of the discrete Q-learning agent, the training approach tries to memorize the track as it progresses, which is motivated by how human players perform when playing the game repeatedly.

In testing, the \texttt{hd()} resolution is used for visualization. $\epsilon$ is set to 0 for deterministic policy. To extract the best performance of each method, last 10 saved Q-tables from episodes 2910 to 3000 are used for each track, and the best performance is selected as shown in Table \ref{best_performance}. Plot figures of the last saved episodes are shown in Appendix \ref{qlearning_plots}.


\begin{table}[h]
    \caption{Best Steps taken performance of each method on each track}
    \label{best_performance}
    \centering
    \begin{tabular}{llll}
        \toprule
        Method              & Zengarden    & Lighthouse      & Hacienda  \\
        \midrule
        Simple Q-learning   & 547          & \textbf{491}    & \textbf{567}  \\
        Discrete Q-learning & \textbf{528} & 595             & 806 (95\%)  \\
        Manual Controller   & 631          & 503             & 649  \\
        HW Target & \underline{500}        & \underline{500} & \underline{700}  \\
        \bottomrule
        \toprule
        Method              & Snowtuxpeak  & Cornfield\_crossing & Scotland  \\
        \midrule
        Simple Q-learning   & \textbf{518} & \textbf{711}    & \textbf{660}  \\
        Discrete Q-learning & 782          & 842             & 771 (90\%)  \\
        Manual Controller   & 632          & 764             & 696  \\
        HW Target & \underline{600}        & \underline{700} & \underline{700}  \\
        \bottomrule
    \end{tabular}
\end{table}


Table \ref{best_performance} compares the best performance of Q-learning agents against a manually pre-defined controller and the target performance given by the homework assignment. The manual controller is set with the follwing pre-defined policy:

\begin{itemize}
    \item \texttt{steer} = 1 if $x$ > 0, else -1 if $x$ < 0, else 0
    \item (\texttt{accel}, \texttt{brake}, \texttt{drift}, \texttt{nitro}) = (0.8, False, False, False)
\end{itemize}

where $x$ is the x-coordinate of the aim point ranging from -1 to 1.

From the table, we can see that the simple Q-learning agent performs better than the manual controller and the discrete Q-learning agent as well. Except for the Zengarden track, the discrete Q-learning agent performs better than the simple Q-learning agent. However, the simple Q-learning agent does not defeat the homework target performance in Zengarden and Cornfield\_crossing tracks. The discrete Q-learning agent is able to defeat the simple Q-learning agent in Zengarden but fails to finish the race in Hacienda and Scotland tracks.

\subsubsection{Local vs Ibex}

When testing the Q-learning agents locally after training on Ibex, the results are a little bi different. For example, the discrete Q-learning agent is able to finish the race in Scotland track locally but not on Ibex. 

\begin{table}[h]
    \caption{Performance comparison between local and Ibex}
    \label{local_vs_ibex}
    \centering
    \begin{tabular}{llll}
        \toprule
        Method              & Zengarden    & Lighthouse      & Hacienda  \\
        \midrule
        Ibex Simple Q-learning   & 547          & \textbf{491} & 567  \\
        Ibex Discrete Q-learning & 528          & 595          & 806 (95\%)  \\
        Local Simple Q-learning   & \textbf{477} & 501         & \textbf{550}  \\
        Local Discrete Q-learning & 630         & 577          & 779 (gets stuck)  \\
        \bottomrule
        \toprule
        Method              & Snowtuxpeak  & Cornfield\_crossing & Scotland  \\
        \midrule
        Ibex Simple Q-learning   & \textbf{518} & 711    & \textbf{660}  \\
        Ibex Discrete Q-learning & 782          & 842             & 771 (90\%)  \\
        Local Simple Q-learning   & 611          & \textbf{693}   & \textbf{660}  \\
        Local Discrete Q-learning & 788          & 837             & 730  \\
        \bottomrule
    \end{tabular}
\end{table}

Table \ref{local_vs_ibex} shows the performance comparison between local and Ibex tests. The better performance results alternate between local and Ibex results. In the case of the simple Q-learning agent, the local agent was better in Zengarden, Hacienda, and Cornfield\_crossing tracks, and the Ibex agent was better in Lighthouse ans Snowtuxpeak while performing similar in Scotland track. Similar alternating results are observed for the discrete Q-learning agent. This suggests that the computations handled in my local machine and Ibex differ. 

\subsubsection{Function Approximation}

Per the presentation comments, I implemented a simple function approximation model in \texttt{neural\_RL\_controller.py}. The model is a simple feedforward neural network with 3 layers:

\begin{itemize}
    \item Input layer: The input layer takes a state similar to the discrete Q-learning agent as input.
    \item Hidden layer: The hidden layer has 64 units.
    \item Output layer: The output layer has 72 units.
\end{itemize}

The model approximates the Q-value function for the action space of the discrete Q-learning agent. In addition, a memory buffer is used to store the experiences of the agent and provide samples for training, and the reward function has been modified to include a reward of $0.01$ for each time the kart's distance down the track gets larger. However, the model does not show any improvements, and, in fact, the model does not even finish the race in the Zengarden track. Figure \ref{approx_reward} shows the cumulative reward of the function approximation model during training on the Zengarden track. The rewards are in the negative range, indicating that the model does not make any progress.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{approx_reward.png}
    \caption{Cumulative reward of function approximation model on Zengarden track (5000 episodes)}
    \label{approx_reward}
\end{figure}


\subsection{DQN}
Similar to the training setup of Q-learning, DQN is trained with a maximum of 2000 steps per episode and an $\epsilon$-greedy policy is used for exploration. Images are loaded with the \texttt{hd()} resolution to reflect the actual game. However, even after training for 5000 episodes, the DQN agent does not reach the finish line in any run and all cumulative rewards reported are negative, which indicates that the DQN agent is not able to learn the game and gets stuck in a loop.

Figure \ref{DQN_reward} shows the normalized reward per step of DQN agent on all tracks. The reward is scaled by the number of steps taken in the episode. The majority of the rewards are in the negative range, indicating that the DQN agent gets stuck and does not make any progress. Figure \ref{DQN_loss} shows the normalized loss per step of DQN agent on all tracks. The loss is scaled by the number of steps taken in the episode. The loss figure shows different behavior for different tracks. For example, the normalized loss of Lighthouse and Cornfield\_crossing tracks are similar, where the loss fluctuates around 0.007 within the range of 0.002 to 0.012. However, the loss Zengarden, Snowtuxpeak, and Scotland tracks are much higher and experiencing more fluctuations in larger ranges with many spikes. Nonetheless, none of the episodes have finished the race during training and the DQN agent is not able to learn the game.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{reward.png}
    \caption{Normalized reward per step of DQN agent training on all tracks}
    \label{DQN_reward}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{loss.png}
    \caption{Normalized loss per step of DQN agent training on all tracks}
    \label{DQN_loss}
\end{figure}


\section{Conclusion}

In this project, I implemented a simple Q-learning agent, a discrete Q-learning agent, a function approximation model, and a DQN agent. The simple Q-learning agent is able to finish the race in all tracks, and the discrete Q-learning agent is able to finish the race in all tracks except for Hacienda and Scotland tracks. The function approximation model does not show any improvements, and the DQN agent is not able to learn the game and gets stuck in a loop. The simple Q-learning agent does not defeat the homework target performance in Zengarden and Cornfield\_crossing tracks. The discrete Q-learning agent is able to defeat the simple Q-learning agent in Zengarden but fails to defeat the homework target performance in all other tracks. For the DQN agent, some hyperparameters tuning have been done, but the model needs further tuning to learn the game. Further, the reward function needs to be tuned to encourage the kart to move forward and finish the race, but this was not successfully done in this project.


\begin{thebibliography}{2}

\bibitem[Kr채henb체hl(2021)]{pystk}
Kr채henb체hl, P. (2021). pystk Documentation (Version 1.0) [Computer software documentation]. Retrieved from https://readthedocs.org/projects/pystk/downloads/pdf/latest/

\bibitem[Mnih et al.(2013)]{mnih}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., \& Riedmiller, M. (2013) Playing Atari with Deep Reinforcement Learning. {\it arXiv preprint arXiv:1312.5602}.

\bibitem[Mnih et al.(2015)]{mnih2}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... \& Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533. https://doi.org/10.1038/nature14236

\bibitem[Orabona(2025)]{lecnotes}
Orabona, F. (2025) CS294X: Introduction to Reinforcement Learning Lecture Notes. {\it King Abdullah University of Science and Technology}.

\end{thebibliography}


\appendix

\section{Q-learning Plots}
\label{qlearning_plots}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{simple_RL_controller_reward_distance.png}
    \caption{Reward distance of simple Q-learning agent on all tracks}
    \label{simple_RL_controller_reward_distance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{simple_RL_controller_steps_only.png}
    \caption{Steps taken of simple Q-learning agent on all tracks}
    \label{simple_RL_controller_steps_only}
\end{figure}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{discrete_RL_controller_reward_distance.png}
    \caption{Reward distance of discrete Q-learning agent on all tracks}
    \label{discrete_RL_controller_reward_distance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{discrete_RL_controller_steps_only.png}
    \caption{Steps taken of discrete Q-learning agent on all tracks}
    \label{discrete_RL_controller_steps_only}
\end{figure}

\end{document}